# 疑惑汇总(待整理)

## 关于正则化

batchnorm计算的是不同通道下本batch的所有样本的均值方差.

那输入应该是什么？对于一个batch, X([N, C, H, W]), 应该输入什么？

spring1718_assignment2_v2/BatchNormalization.ipynb中的这部分, 以随机样本进行计算, 如果对于图像而言, 真正的输入应该是什么样子的？

```python
np.random.seed(231)
# 可以理解为200张50*60*3的照片集合
N, D1, D2, D3 = 200, 50, 60, 3
# 输入的数据被归置到前两维度上面, 这里怎么理解？对于一副真实图像, 如何理解此处对应的X
X = np.random.randn(N, D1)
W1 = np.random.randn(D1, D2)
W2 = np.random.randn(D2, D3)
# affine-relu-affine
a = np.maximum(0, X.dot(W1)).dot(W2)

print('Before batch normalization:')
print_mean_std(a,axis=0)

# 这里为什么要使用D3来初始化长度, 可以从后面计算的过程中看出来, gamma也是针对各个特征有一个gamma, beta
gamma = np.ones((D3,))
beta = np.zeros((D3,))

# Means should be close to zero and stds close to one
print('After batch normalization (gamma=1, beta=0)')
a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})
print_mean_std(a_norm,axis=0)

gamma = np.asarray([1.0, 2.0, 3.0])
beta = np.asarray([11.0, 12.0, 13.0])
# Now means should be close to beta and stds close to gamma
# 重构之后, 得到的均值标准差和theta,gamma相近
print('After batch normalization (gamma=', gamma, ', beta=', beta, ')')
a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})
print_mean_std(a_norm,axis=0)
```

在layers.py文件中的batchnorm_forward函数中, 有这样的代码：

```python
mode = bn_param['mode']
eps = bn_param.get('eps', 1e-5)
momentum = bn_param.get('momentum', 0.9)

N, D = x.shape
running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))
running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))

out, cache = None, None

...

# 均值的计算, 是计算N方向上的, 也就是算的是所有样本对于同一特征的均值
sample_mean = np.mean(x, axis=0)
# 计算标准差（这之中使用了参数epsilon)
sample_var = np.var(x, axis=0)
# 动量方式滑动更新策略
running_mean = momentum * running_mean + (1 - momentum) * sample_mean
running_var = momentum * running_var + (1 - momentum) * sample_var
# 计算归一化值
x_normed = (x - sample_mean) / np.sqrt(sample_var + eps)
# 变换重构
out = gamma * x_normed + beta
# 保存参数
cache = (x, gamma, beta, x_normed, sample_mean, sample_var, eps)
```

从上面的过程来看, 应该是输入成二维的数据, 对于**批量的三维图像**而言, 可能会有转化为 [N,(H×W×C)]的一个二维输入. 看下后面在类里的使用方式：

```python
np.random.seed(231)
N, D, H1, H2, C = 2, 15, 20, 30, 10
X = np.random.randn(N, D)
y = np.random.randint(C, size=(N,))

# You should expect losses between 1e-4~1e-10 for W,
# losses between 1e-08~1e-10 for b,
# and losses between 1e-08~1e-09 for beta and gammas.
for reg in [0, 3.14]:
    print('Running check with reg = ', reg)
    model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,
                              reg=reg, weight_scale=5e-2, dtype=np.float64,
                              normalization='batchnorm')

    loss, grads = model.loss(X, y)
    print('Initial loss: ', loss)

    for name in sorted(grads):
        f = lambda _: model.loss(X, y)[0]
        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)
        print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))
    if reg == 0: print()
```

这里涉及到BN的用法和上面一样.

为了找到更实际的, 找到了这部分内容：

```python
np.random.seed(231)
# Try training a very deep net with batchnorm
hidden_dims = [100, 100, 100, 100, 100]

num_train = 1000
# data：
# X_train:  (49000, 3, 32, 32)
# y_train:  (49000,)
# X_val:  (1000, 3, 32, 32)
# y_val:  (1000,)
# X_test:  (1000, 3, 32, 32)
# y_test:  (1000,)
small_data = {
  'X_train': data['X_train'][:num_train],
  'y_train': data['y_train'][:num_train],
  'X_val': data['X_val'],
  'y_val': data['y_val'],
}
# X_train: (1000, 2, 32, 32)
# y_train: (1000, )
# ...

weight_scale = 2e-2

bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization='batchnorm')
bn_solver = Solver(bn_model, small_data,
                num_epochs=10, batch_size=50,
                update_rule='adam',
                optim_config={
                  	'learning_rate': 1e-3,
                },
                verbose=True,print_every=20)
bn_solver.train()

model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=None)
solver = Solver(model, small_data,
                num_epochs=10, batch_size=50,
                update_rule='adam',
                optim_config={
                  	'learning_rate': 1e-3,
                },
                verbose=True, print_every=20)
solver.train()
```

查看Solver类对应的model.loss()向调用的函数查找, 找到了：

fc_net.py中的全连接类的

![1537407981290](assets/1537407981290.png)

```python
def loss(self, X, y=None):
    """
    Compute loss and gradient for a minibatch of data.
    Inputs:
    - X: Array of input data of shape (N, d_1, ..., d_k)
    - y: Array of labels, of shape (N,). y[i] gives the label for X[i].

    Returns:
    If y is None, then run a test-time forward pass of the model and return:
    - scores: Array of shape (N, C) giving classification scores, where
      scores[i, c] is the classification score for X[i] and class c.
    If y is not None, then run a training-time forward and backward pass and
    return a tuple of:
    - loss: Scalar value giving the loss
    - grads: Dictionary with the same keys as self.params, mapping parameter
      names to gradients of the loss with respect to those parameters.
    """
    scores = None
    ############################################################################
    # TODO: Implement the forward pass for the two-layer net, computing the    #
    # class scores for X and storing them in the scores variable.              #
    ############################################################################
    # affine_1 W1 input*hidden
    #  affine_1_out = X.dot(self.params['W1']) + self.params['b1']
    # relu_1
    #  relu_1_out = np.maximum(0, affine_1_out)
    # affine_2 W2 hidden*classes
    #  affine_2_out = relu_1_out.dot(self.params['W2']) + self.params['b2']
    # 最后的softmax不计算, 因为得分的计算计算到最后的分类之前即可
    #  scores = affine_2_out
    # 思路是上面的思路, 但是实际计算的时候需要注意具体的数据形式, 这里还是用已有
    # 的函数计算比较方便
    layer_1, cache_1 = affine_relu_forward(
        X, self.params['W1'], self.params['b1'])
    layer_2, cache_2 = affine_forward(
        layer_1, self.params['W2'], self.params['b2'])
    scores = layer_2
```
再看affine_relu_forward中调用了affine_forward, 找到了：

```python
N = x.shape[0]
# 将多维e的x归置到二维, 各个样本的数据都被压缩到一个维度
new_x = x.reshape([N, -1])
out = np.dot(new_x, w) + b
```
原来, 对于batchnorm而言, 得到的batch数据x是二维数据, N×D, D=H×W×C, 所以再看batchnorm_forward函数：

```python
sample_mean = np.mean(x, axis=0)
# 计算标准差(这之中使用了参数epsilon)
sample_var = np.var(x, axis=0)
# 动量方式滑动更新策略
running_mean = momentum * running_mean + (1 - momentum) * sample_mean
running_var = momentum * running_var + (1 - momentum) * sample_var
# 计算归一化值
x_normed = (x - sample_mean) / np.sqrt(sample_var + eps)
# 变换重构
out = gamma * x_normed + beta
# 保存参数
cache = (x, gamma, beta, x_normed, sample_mean, sample_var, eps)
```
得到的均值方差都是各个特征的对应的均值方差.

也就实际上是说, 对于batchnorm而言, 实际上是对一个batch而言, 求得各个特征的均值方差, 对各个特征进行归一化.

而对于layernorm而言, 实际上略有不同：

```python
# 因为layernorm是关于特征的处理, x到这里的时候, 已经是二维的了.
x_new = x.copy()
# 这里的计算均值方差, 是针对一个样本而言
sample_mean = np.mean(x_new, axis=1)
sample_var = np.var(x_new, axis=1)

# 这里是按照不同的样本进行的均值方差计算
x_new = x_new - sample_mean.reshape([-1, 1])
x_new = x_new / np.sqrt(sample_var.reshape([-1, 1]) + eps)
out = gamma * x_new + beta
cache = (x, gamma, beta, x_new, sample_mean, sample_var, eps)
```
可见这里的均值方差与batchnorm计算方向不同, 不过这里也使用了batch的方式来计算, 只是计算方向不同, 所以与N无关, 而batchnorm与N紧密关联.

而对于spatial_batchnorm_forward而言, 它的输入是四维的batch, 它是把四维里的除了C维度的三维压缩到一起.

```python
N, C, H, W = x.shape
# 调整x到 N×H×W × C 大小, 四维压缩到二维, 保留C维度
# np.transpose可以调整各维度的顺序, 对于二维而言也就是转置了
x_new = np.reshape(np.transpose(x, (0, 2, 3, 1)), (-1, C))

out, cache = batchnorm_forward(x_new, gamma, beta, bn_param)

# 调整out从(N, H, W, C)到(N, C, H, W)
out = np.transpose(np.reshape(out, (N, H, W, C)), (0, 3, 1, 2))
```
SBN运算相当于它是在C方向上划分了数据, 对于图像而言, 就是划分了三层, RGB各一层. 再进行BN运算.

原始的BN运算, 是在N方向上划分数据, 对于图像batch而言, 就是划分了多个样本, 一个样本一层.

对于作业中出现的spatial_groupnorm_forward而言, 它的输入是四维的, 但是在压缩调整的时候, 在C的方向上进行了分组, 分成G组, 一组也看做一个样本, 从而有N×G个样本, 再进行BN.

> C方向一般不是指代通道数目方向么？那一般不就是3么？
>
> 但是这个输入可能不是最一开始的输入,而是卷积后的输入,这会使得channel数目不再为3或者4.

```python
N, C, H, W = x.shape
# GroupNorm：将channel方向分group, 然后每个group内做归一化, 算(C//G)*H*W的均值
x = np.reshape(x, (N * G, C // G * H * W))

x = x.T
mu = np.mean(x, axis=0)
xmu = x - mu
sq = xmu ** 2
var = np.var(x, axis=0)

sqrtvar = np.sqrt(var + eps)
ivar = 1. / sqrtvar
xhat = xmu * ivar

xhat = np.reshape(xhat.T, (N, C, H, W))
out = gamma[np.newaxis, :, np.newaxis, np.newaxis] * \
    xhat + beta[np.newaxis, :, np.newaxis, np.newaxis]
cache = (xhat, gamma, xmu, ivar, sqrtvar, var, eps, G)
```

## 关于特征可视化

> 然而![x_i ](https://www.zhihu.com/equation?tex=x_i%0A) 的梯度有时仍然是有用的：比如**将神经网络所做的事情可视化便于直观理解的时候**, 就能用上.

怎么理解?怎么用?

## 关于白化

> **最右**：将“白化”后的数据进行显示. 其中144个维度中的方差都被压缩到了相同的数值范围. 然后144个白化后的数值通过乘以`U.transpose()[:144,:]`转换到图像像素基准上. 现在较低的频率(代表了大多数方差）可以忽略不计了, 较高的频率(代表相对少的方差）就被夸大了.

> “现在较低的频率(代表了大多数方差)可以忽略不计了, 较高的频率(代表相对少的方差)就被夸大了”这句话的含义？
>
> 为什么较低的频率代表了大多数的方差？又为什么可以忽略不计？
>
> 为什么较高的频率代表相对较少的方差？

## 关于神经网络权重小随机数初始化

> 因此, 权重初始值要非常接近0又不能等于0. 解决方法就是将权重初始化为很小的数值, 以此来*打破对称性*. 其思路是：如果神经元刚开始的时候是随机且不相等的, 那么它们将计算出不同的更新, 并将自身变成整个网络的不同部分.
>
> 小随机数权重初始化的实现方法是：`W = 0.01 * np.random.randn(D,H)`. **其中**`randn`**函数是基于零均值和标准差的一个高斯分布**(**译者注：国内教程一般习惯称均值参数为期望![\mu](https://www.zhihu.com/equation?tex=%5Cmu)**)来生成随机数的. 根据这个式子, 每个神经元的权重向量都被初始化为一个随机向量, 而这些随机向量又服从一个多变量高斯分布, 这样在输入空间中, 所有的神经元的指向是随机的. 也可以使用均匀分布生成的随机数, 但是从实践结果来看, 对于算法的结果影响极小.
>
> **警告**. 并不是小数值一定会得到好的结果. 例如, 一个神经网络的层中的权重值很小, 那么在反向传播的时候就会计算出非常小的梯度(因为梯度与权重值是成比例的). 这就会很大程度上减小反向传播中的“梯度信号”, 在深度网络中, 就会出现问题.

> 如何理解“梯度与权重值是成比例的”？

## 关于在操作的特性模式中梯度检查

> 有一点必须要认识到：梯度检查是在参数空间中的一个特定(往往还是随机的)的单独点进行的. 即使是在该点上梯度检查成功了, 也不能马上确保全局上梯度的实现都是正确的.
>
> 还有, **一个随机的初始化可能不是参数空间最优代表性的点**, 这可能导致进入某种病态的情况, 即梯度看起来是正确实现了, 实际上并没有. 例如, SVM使用小数值权重初始化, 就会把一些接近于0的得分分配给所有的数据点, 而梯度将会在所有的数据点上展现出某种模式.

> “SVM使用小数值权重初始化, 就会把一些接近于0的得分分配给所有的数据点, 而梯度将会在所有的数据点上展现出某种模式”怎么理解？

## 关于超参数调优中随机搜索优于网格搜索

Bergstra和Bengio在文章[Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)中说“随机选择比网格化的选择更加有效”, 而且在实践中也更容易实现.

![img](https://pic4.zhimg.com/80/d25cf561835c7b96ae6d1c91868bcbff_hd.png)

在 Random Search for Hyper-Parameter Optimization 中的核心说明图. 通常, 有些超参数比其余的更重要, 通过随机搜索, 而不是网格化的搜索, 可以让你更精确地发现那些比较重要的超参数的好数值.

> 何为网格化搜索？具体怎么实现？

## 关于神经网络评价的模型集成

>  在实践的时候, 有一个总是能提升神经网络几个百分点准确率的办法, 就是在训练的时候训练几个独立的模型, 然后在测试的时候平均它们预测结果. **集成的模型数量增加, 算法的结果也单调提升(但提升效果越来越少)**. 还有模型之间的差异度越大, 提升效果可能越好.

> 测试的时候平均预测结果？参数不是都不一样了么？可以平均么？
>
> 这个集成过程的详细流程是怎样的？
>

## 关于softmax的损失函数

> 在Softmax分类器中, 函数映射![f(x_i;W)=Wx_i](https://www.zhihu.com/equation?tex=f%28x_i%3BW%29%3DWx_i)保持不变, 但将这些评分值视为每个分类的未归一化的对数概率, 并且将*折叶损失(hinge loss)*替换为**交叉熵损失**(**cross-entropy loss)**. 公式如下：
>
> ![\displaystyle Li=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29) 或等价的 ![L_i=-f_{y_i}+log(\sum_je^{f_j})](https://www.zhihu.com/equation?tex=L_i%3D-f_%7By_i%7D%2Blog%28%5Csum_je%5E%7Bf_j%7D%29)
>
> 总的损失可以记为：$L=\frac{1}{N}\sum_iL_i$
>
> 其中函数![1538105944188](assets/1538105944188.png)被称作**softmax(柔性最大值) 函数**：其输入值是一个向量, 向量中元素为任意实数的评分值(![z](https://www.zhihu.com/equation?tex=z)中的, 也就是$z_j=\theta_j^Tx$), 函数对其进行压缩, 输出一个向量, 其中每个元素值在0到1之间, 且所有元素之和为1.
>
> **概率论解释**：先看下面的公式：
>
> ![P(y_i|x_i,W)=\frac{e^{f_{y_i}}}{\sum_je^{f_j}}](https://www.zhihu.com/equation?tex=P%28y_i%7Cx_i%2CW%29%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D)
>
> 可以解释为是给定图像数据![x_i](https://www.zhihu.com/equation?tex=x_i), 以![W](https://www.zhihu.com/equation?tex=W)为参数, 分配给正确分类标签![y_i](https://www.zhihu.com/equation?tex=y_i)的归一化概率.
>
> 为了理解这点, 请回忆一下Softmax分类器将**输出向量![f](https://www.zhihu.com/equation?tex=f)中的评分值解释为没有归一化的对数概率**. 那么以这些数值做指数函数的幂就得到了没有归一化的概率, 而除法操作则对数据进行了归一化处理, 使得这些概率的和为1.
>
> 从概率论的角度来理解, 我们就是在最小化正确分类的负对数概率, 这可以看做是在进行*最大似然估计*(MLE).
>
> 该解释的另一个好处是, 损失函数中的正则化部分![R(W)](https://www.zhihu.com/equation?tex=R%28W%29)可以被看做是权重矩阵![W](https://www.zhihu.com/equation?tex=W)的高斯先验, 这里进行的是最大后验估计(MAP)而不是最大似然估计. 提及这些解释只是为了让读者形成直观的印象, 具体细节就超过本课程范围了.

> 这里并不理解, 为何可以看作是高斯先验？
>
> 为何进行的是最大后验估计？
>
> 最大后验估计最大似然估计如何判断？
