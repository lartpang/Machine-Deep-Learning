# Deep Image Matting

> <http://arxiv.org/pdf/1703.03872v3.pdf>

* [Deep Image Matting](#deep-image-matting)
  * [Abstract](#abstract)
  * [Introduction](#introduction)
  * [Related works](#related-works)
  * [New matting dataset](#new-matting-dataset)
  * [Our method](#our-method)
    * [Matting encoder-decoder stage](#matting-encoder-decoder-stage)
      * [Network structure](#network-structure)
      * [Losses](#losses)
        * [alpha-prediction loss](#alpha-prediction-loss)
        * [compositional loss](#compositional-loss)
      * [Implementation](#implementation)
        * [Train](#train)
        * [Test](#test)
    * [Matting refinement stage](#matting-refinement-stage)
      * [Network structure](#network-structure-1)
      * [Implementation](#implementation-1)
  * [Experimental results](#experimental-results)
    * [The alphamatting.com dataset](#the-alphamattingcom-dataset)
    * [The Composition-1k testing dataset](#the-composition-1k-testing-dataset)
    * [The real image dataset](#the-real-image-dataset)
  * [Conclusion](#conclusion)

## Abstract

Image matting is a fundamental computer vision problem and has many applications. Previous algorithms have poor performance *when an image has similar foregroundand background colors or complicated textures*. The main reasons are prior methods

1. only use low-level features
2. lack high-level context

In this paper, we propose a novel deep learning based algorithm that can tackle both these problems. Our deep model has two parts.

* The first part is **a deep convolutional encoder-decoder network** that takes an image and the corresponding trimap as inputs and *predict the alpha matte of the image*.
* The second part is **a small convolutional network** that *refines the alpha matte predictions* of the first network to have more accurate alpha values and sharper edges.

In addition, we also create a large-scale image matting dataset including **49300 training images and 1000 testing images**. We evaluate our algorithm on the image matting benchmark, our testing set, and a wide variety of real images. Experimental results clearly demonstratethe superiority of our algorithm over previous methods.

## Introduction

Matting, the problem of accurate foreground estimationin images and videos, has significant practical importance. It is a key technology in image editing and film production and effective natural image matting methods can greatly improve current professional workflows. It necessitates(å¿…è¦) methods that handle real world images in unconstrained scenes.

Unfortunately, current matting approaches do not generalize well to typical everyday scenes. This is partially due to the difficulty of the problem: as formulated the matting problem is underconstrained(æ¬ çº¦æŸ) with 7 unknown values per pixel but only 3 known values:

![img](assets/2018-12-31-17-28-49.png)

where the RGB color at pixel $i$, $I_i$, is known and the foreground color $F_i$, background color $B_i$ and matte estimation $Î±_i$ are unknown. However, current approaches are further limited in their approach.

1. The first limitation is due to current methods being designed to solve the matting equation (Eq.1). This equation formulates the matting problem as a linear combination of two colors, and consequently most current algorithms approach this largely as a color problem(å› æ­¤ï¼Œå¤§å¤šæ•°ç°æœ‰ç®—æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°†å…¶ä½œä¸ºé¢œè‰²é—®é¢˜).

    The standard approaches include sampling foreground and back-ground colors [3,9], propagating the alpha values according to the matting equation [14,31,22], or a hybrid of the two [32,13,28,16].

    è¿™äº›æ–¹æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºcolorä½œä¸ºåŒºåˆ«ç‰¹å¾ï¼ˆé€šå¸¸ä¸åƒç´ çš„ç©ºé—´ä½ç½®ä¸€èµ·ï¼‰ï¼Œä½¿å¾—å®ƒä»¬åœ¨å‰æ™¯å’ŒèƒŒæ™¯é¢œè‰²åˆ†å¸ƒé‡å çš„æƒ…å†µä¸‹éå¸¸æ•æ„Ÿï¼Œä¸å¹¸çš„æ˜¯ï¼Œå¯¹äºè¿™äº›æ–¹æ³•, è¿™äº›é—®é¢˜æ˜¯è‡ªç„¶å›¾åƒä¸­çš„å¸¸è§æƒ…å†µï¼Œé€šå¸¸å¯¼è‡´ä¾èµ–äºè¯¥æ–¹æ³•çš„ä½é¢‘â€œæ‹–å°¾â€æˆ–é«˜é¢‘â€œåšå®â€ä¼ªå½±(æ‰€è°“å¼‚å¸¸)ï¼ˆå‚è§å›¾1çš„é¡¶è¡Œï¼‰ã€‚å³ä½¿æœ€è¿‘æå‡ºçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ä¹Ÿé«˜åº¦ä¾èµ–äºä¸é¢œè‰²ç›¸å…³çš„ä¼ æ’­æ–¹æ³• [8,29].

2. A second limitation is due to the focus on a very small dataset.

    * Generating ground truth for matting is very difficult, and the alphamatting.com dataset [25] made a significant contribution to matting research by providing ground-truth data. Unfortunately, it contains only 27 training images and 8 test images, most of which are objects in front of an image on a monitor. Due to its size and constraints(é™åˆ¶) of the dataset (e.g. indoor lab scenes, indoor lighting, no humans or animals), it is by its nature biased(å®ƒæœ¬è´¨ä¸Šæ˜¯æœ‰åè§çš„), and methods are incentivized to fit to this data for publication purposes(ä¸ºäº†å‘å¸ƒç›®çš„ï¼Œé¼“åŠ±æ–¹æ³•é€‚åˆè¿™äº›æ•°æ®). As is the case with all datasets, especially small ones, at some point methods will overfit to the dataset and no longer generalize to real scenes.
    * A recent video matting dataset is available [10] with 3 training videos and 10 test videos, 5 of which were extracted from green screen footage(ç»¿å±ç”»é¢) and the rest using a similar method to [25].

In this work, we present an approach aimed to overcome these limitations. Our method uses deep learning to **directly compute the alpha matte given an input image and trimap**. Instead of relying primarily on color information, our network can **learn the natural structure that is present in alpha mattes**. For example, hair and fur (which usually require matting) possess strong structural and textural pat-terns. Other cases requiring matting (e.g. edges of objects, regions of optical or motion blur, or semitransparent regions) almost always have a common structure or alpha profile that can be expected. While low-level features will not capture this structure, deep networks are ideal for representing it. Our two-stage network includes **an encoder-decoder stage followed by a small residual network for refinement** and includes **a novel composition loss in addition to a loss on the alpha**. We are the first to demonstrate the ability to learn an alpha matte end-to-end given an image and trimap.

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¨åœ¨å…‹æœè¿™äº›å±€é™æ€§çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•**ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¥ç›´æ¥è®¡ç®—è¾“å…¥å›¾åƒå’Œtrimapçš„alphaé®ç½©**ã€‚æˆ‘ä»¬çš„ç½‘ç»œ**ä¸æ˜¯ä¸»è¦ä¾èµ–äºé¢œè‰²ä¿¡æ¯ï¼Œè€Œæ˜¯å¯ä»¥å­¦ä¹ alphaé®ç½©ä¸­å­˜åœ¨çš„è‡ªç„¶ç»“æ„**ã€‚ä¾‹å¦‚ï¼Œå¤´å‘å’Œæ¯›çš®ï¼ˆé€šå¸¸éœ€è¦æ¶ˆå…‰ï¼‰å…·æœ‰å¼ºçƒˆçš„ç»“æ„å’Œçº¹ç†å›¾æ¡ˆã€‚éœ€è¦æ¶ˆå…‰çš„å…¶ä»–æƒ…å†µï¼ˆä¾‹å¦‚ï¼Œç‰©ä½“çš„è¾¹ç¼˜ï¼Œå…‰å­¦æˆ–è¿åŠ¨æ¨¡ç³ŠåŒºåŸŸæˆ–åŠé€æ˜åŒºåŸŸï¼‰å‡ ä¹æ€»æ˜¯å…·æœ‰å¯ä»¥é¢„æœŸçš„å…±åŒç»“æ„æˆ–Î±æ–‡ä»¶ã€‚è™½ç„¶ä½çº§åŠŸèƒ½æ— æ³•æ•è·æ­¤ç»“æ„ï¼Œä½†æ·±åº¦ç½‘ç»œéå¸¸é€‚åˆè¡¨ç¤ºå®ƒã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µç½‘ç»œåŒ…æ‹¬ç¼–ç å™¨-è§£ç é˜¶æ®µï¼Œåé¢æ˜¯ä¸€ä¸ªå°çš„æ®‹å·®ç½‘ç»œï¼Œç”¨äºç»†åŒ–ï¼Œ**å¹¶ä¸”é™¤äº†åœ¨alphaä¸Šçš„æŸå¤±ä¹‹å¤–, è¿˜åŒ…æ‹¬æ–°çš„æˆåˆ†æŸå¤±**ã€‚æˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªå±•ç¤ºèƒ½å¤Ÿåœ¨ç»™å®šå›¾åƒå’Œtrimapçš„æƒ…å†µä¸‹ç«¯åˆ°ç«¯çš„alphaé®ç½©çš„èƒ½åŠ›ã€‚

To train a model that will excel in natural images of unconstrained scenes, we need a much larger dataset than currently available. Obtaining a ground truth dataset using the method of [25] would be very costly and cannot handle scenes with any degree of motion (and consequently cannot capture humans or animals). Instead, inspired by other synthetic datasets that have proven sufficient to train models for use in real images (e.g. [4]), we create a large-scale matting dataset using composition.

ä¸ºäº†è®­ç»ƒä¸€ä¸ªåœ¨æ— çº¦æŸåœºæ™¯çš„è‡ªç„¶å›¾åƒä¸­è¡¨ç°ä¼˜å¼‚çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¯”ç°æœ‰æ›´å¤§çš„æ•°æ®é›†ã€‚ä½¿ç”¨[25]çš„æ–¹æ³•è·å¾—çœŸå®æ ‡æ³¨æ•°æ®é›†å°†æ˜¯éå¸¸æ˜‚è´µçš„ï¼Œå¹¶ä¸”æ— æ³•å¤„ç†ä»»ä½•ç¨‹åº¦çš„è¿åŠ¨åœºæ™¯ï¼ˆå› æ­¤æ— æ³•æ•è·äººç±»æˆ–åŠ¨ç‰©ï¼‰ã€‚**ç›¸åï¼Œå—å…¶ä»–åˆæˆæ•°æ®é›†çš„å¯å‘ï¼Œè¿™äº›æ•°æ®é›†å·²è¢«è¯æ˜è¶³ä»¥è®­ç»ƒæ¨¡å‹ä»¥ç”¨äºçœŸå®å›¾åƒï¼ˆä¾‹å¦‚[4]ï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆæˆåˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æ¶ˆå…‰æ•°æ®é›†**ã€‚

Images with objects on simple backgrounds were carefully extracted and were composited onto new background images to create a dataset with 49300 training images and 1000 test images.

ä»”ç»†æå–å…·æœ‰ç®€å•èƒŒæ™¯ä¸Šçš„å¯¹è±¡çš„å›¾åƒå¹¶å°†å…¶åˆæˆåˆ°æ–°çš„èƒŒæ™¯å›¾åƒä¸Šä»¥åˆ›å»ºå…·æœ‰49300ä¸ªè®­ç»ƒå›¾åƒå’Œ1000ä¸ªæµ‹è¯•å›¾åƒçš„æ•°æ®é›†ã€‚

We perform extensive evaluation to prove the effectiveness on our method. Not only does our method achieve first place on the alphamatting.com challenge, but we also greatly outperform prior methods on our synthetic test set. We show our learned model generalizes to natural images with a user study comparing many prior methods on 31 natural images featuring humans, animals, and other objects in varying scenes and under different lighting conditions. This study shows a strong preference for our results, but also shows that some methods which perform well on the alphamatting.com dataset actually perform worse compared to other methods when judged by humans, suggesting that methods are being to overfit on the alphamatting.com test set.

Finally, we also show that we are more robust to trimap placement than other methods. In fact, we can produce great results even when there is no known foreground and/or background in the trimap while most methods cannot return any result (see Fig1 bottom row ).

æˆ‘ä»¬è¿›è¡Œå¹¿æ³›çš„è¯„ä¼°ï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨alphamatting.comæŒ‘æˆ˜ä¸­å æ®é¦–ä½ï¼Œè€Œä¸”æˆ‘ä»¬åœ¨åˆæˆæµ‹è¯•é›†ä¸Šçš„è¡¨ç°ä¹Ÿå¤§å¤§è¶…è¿‡äº†å…ˆå‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„å­¦ä¹ æ¨¡å‹æ¨å¹¿åˆ°è‡ªç„¶å›¾åƒï¼Œç ”ç©¶æ¯”è¾ƒäº†è®¸å¤šä¹‹å‰çš„æ–¹æ³•åœ¨31ç§è‡ªç„¶å›¾åƒä¸Šçš„è¡¨ç°ï¼Œè¿™äº›å›¾åƒä»¥ä¸åŒåœºæ™¯å’Œä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„äººï¼ŒåŠ¨ç‰©å’Œå…¶ä»–ç‰©ä½“ä¸ºç‰¹å¾ã€‚è¿™é¡¹ç ”ç©¶æ˜¾ç¤ºäº†å¯¹äºæˆ‘ä»¬çš„ç»“æœçš„å¼ºçƒˆå€¾å‘ï¼Œä½†ä¹Ÿè¡¨æ˜ï¼Œå½“äººç±»åˆ¤æ–­æ—¶, ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒæŸäº›åœ¨alphamatting.comæ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½çš„æ–¹æ³•å®é™…ä¸Šè¡¨ç°æ›´å·®ï¼Œè¿™è¡¨æ˜è¿™äº›æ–¹æ³•åœ¨alphamatting.comçš„æµ‹è¯•é›†ä¸Šè¿‡åº¦æ‹Ÿåˆäº†ã€‚

æœ€åï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜æˆ‘ä»¬å¯¹trimap placementæ¯”å…¶ä»–æ–¹æ³•æ›´å¼ºå¤§ã€‚å®é™…ä¸Šï¼Œå³ä½¿åœ¨trimapä¸­æ²¡æœ‰å·²çŸ¥çš„å‰æ™¯å’Œ/æˆ–èƒŒæ™¯ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥äº§ç”Ÿå¾ˆå¥½çš„ç»“æœï¼Œè€Œå¤§å¤šæ•°æ–¹æ³•éƒ½ä¸èƒ½è¿”å›ä»»ä½•ç»“æœï¼ˆå‚è§å›¾1åº•è¡Œï¼‰ã€‚

![img](assets/2018-12-31-18-22-57.png)

## Related works

Current matting methods rely primarily on color to determine the alpha matte, along with positional or other low-level features. They do so through **sampling, propagation, or a combination of the two**.

1. In sampling-based methods [3,9,32,13,28,16], **the known foreground and background regions are sampled to find candidate colors for a given pixelâ€™s foreground and background**, then **a metric is used to determine the best foreground/background combination**.

    ä½¿ç”¨ä¸åŒçš„é‡‡æ ·æ–¹æ³•ï¼ŒåŒ…æ‹¬æ²¿æœ€æ¥è¿‘ç»™å®šåƒç´ çš„è¾¹ç•Œé‡‡æ ·[32]ï¼ŒåŸºäºå…‰çº¿æŠ•å°„çš„é‡‡æ ·[13]ï¼Œæœç´¢æ•´ä¸ªè¾¹ç•Œ[16]ï¼Œæˆ–ä»é¢œè‰²èšç±»é‡‡æ ·[28,12](sampling along the boundary nearest the given pixel [32], sampling based on ray casting [13], searching the entire boundary [16], or sampling from color clusters [28,12])ã€‚åœ¨é‡‡æ ·å€™é€‰è€…ä¸­å†³å®šçš„åº¦é‡å‡ ä¹æ€»æ˜¯åŒ…æ‹¬æ¶ˆå…‰æ–¹ç¨‹é‡å»ºè¯¯å·®(a matting equation reconstruction error)ï¼Œå¯èƒ½åŒ…æ‹¬æµ‹é‡æ¥è‡ªç»™å®šåƒç´ çš„æ ·æœ¬çš„è·ç¦»[32,16]æˆ–å‰æ™¯å’ŒèƒŒæ™¯æ ·æœ¬çš„ç›¸ä¼¼æ€§[32,28]ï¼Œä»¥åŠåŒ…æ‹¬ç¨€ç–ç¼–ç [12]å’ŒKL-æ•£åº¦æ–¹æ³•[19,18]çš„å…¬å¼ã€‚è€Œçº¹ç†[27]ç­‰é«˜é˜¶ç‰¹å¾å¾ˆå°‘ä½¿ç”¨ï¼Œæ•ˆæœæœ‰é™ã€‚

2. In propagation methods, Eq.1 is reformulated such that it allows **propagation of the alpha values from the known foreground and background regions into the unknown region**.

    * A popular approach is **Closed-form Matting [22] which is often used as a post-process after sampling** [32,16,28]. It derives a cost function from local smoothness assumption on foreground and background colors and finds the globally optimal alpha matte by solving a sparse linear system of equations(å®ƒä»å‰æ™¯å’ŒèƒŒæ™¯é¢œè‰²çš„å±€éƒ¨å¹³æ»‘å‡è®¾æ¨å¯¼å‡ºæˆæœ¬å‡½æ•°ï¼Œå¹¶é€šè¿‡æ±‚è§£ç¨€ç–çº¿æ€§æ–¹ç¨‹ç»„æ¥æ‰¾åˆ°å…¨å±€æœ€ä¼˜alpha matte).
    * Other propagation methods include **random walks [14], solving Poisson equations [31], and nonlocal propagation methods [21,7, 5]** (éšæœºæ¸¸èµ°[14]ï¼Œæ±‚è§£æ³Šæ¾æ–¹ç¨‹[31]å’Œéå±€éƒ¨ä¼ æ’­æ–¹æ³•[21,7,5]).

Recently, several deep learning works have been proposed for image matting. However, they do not directly learn an alpha matte given an image and trimap.

1. Shenet al. [29] **use deep learning for creating a trimap** of a person in a portrait image and **use [22] for matting** through which matting errors are backpropagated to the network.
2. Choet al.[8] **take the matting results of [22] and [5] and normalized RGB colors as inputs** and learn an end-to-end deep network to **predict a new alpha matte**.

Although both our algorithm and the two works leverage deep learning, our algorithm is quite different from theirs. Our algorithm **directly learns the alpha matte given an image and trimap** while the other two works **rely on existing algorithms to compute the actual matting**, making their methods vulnerable(å¼±åŠ¿) to the same problems as previous matting methods

## New matting dataset

The matting benchmark on alphamatting.com [25] has been tremendously successful in accelerating the pace of research in matting. However, due to the carefully controlled setting required to obtain ground truth images, the dataset consists of only 27 training images and 8 testing images. Not only is this not enough images to train a neural network, but it is severely limited in its diversity(å®ƒçš„å¤šæ ·æ€§å—åˆ°ä¸¥é‡é™åˆ¶), restricted to small-scale lab scenes with static objects.

![img](assets/2018-12-31-18-41-31.png)

To train our matting network, **we create a larger dataset by compositing objects from real images onto new backgrounds**. We find images on simple or plain backgrounds(Fig.2a), including the 27 training images from [25] and every fifth frame from the videos from [26]. Using Photoshop, we carefully manually create an alpha matte (Fig.2b) and pure foreground colors (Fig.2c). Because these object shave simple backgrounds we can pull accurate mattes for them. We then **treat these as ground truth and for each alpha matte and foreground image, we randomly sample N background images in MS COCO [23] and Pascal VOC [11], and composite the object onto those background images**.

We create both a training and a testing dataset in the above way. Our **training dataset has 493 unique foreground objects and 49,300 images (N=100) while ourtesting dataset has 50 unique objects and 1000 images (N=20)**. The trimap for each image is randomly dilated from its ground truth alpha matte. In comparison to previous matting datasets, our new dataset has several advantages.

1. It has many more unique objects and coversvarious matting cases such as hair, fur, semi-transparency,etc.
2. Many composited images have similar foreground and background colors and complex background textures, making our dataset more challenging and practical.

An early concern is whether this process would create a bias due to the composited nature of the images, such that a network would learn to key on differences in the foreground and background lighting, noise levels, etc. However, we found experimentally that we achieved far superior resultson natural images compared to prior methods (see Sec.5.3).

ä¸€ä¸ªæ—©æœŸçš„æ‹…å¿ƒæ˜¯è¿™ä¸ªè¿‡ç¨‹æ˜¯å¦ä¼šå› å›¾åƒçš„åˆæˆæ€§è´¨è€Œäº§ç”Ÿåå·®ï¼Œè¿™æ ·ç½‘ç»œå°±ä¼šå­¦ä¼šå…³æ³¨å‰æ™¯å’ŒèƒŒæ™¯å…‰ç…§ï¼Œå™ªå£°æ°´å¹³ç­‰æ–¹é¢çš„å·®å¼‚ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜äº†ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”è¾ƒ, æˆ‘ä»¬åœ¨è‡ªç„¶å›¾åƒæ–¹é¢å–å¾—äº†å“è¶Šçš„æˆæœã€‚

## Our method

We address the image matting problem using deep learning. Given our new dataset, we train a neural network to fully utilize the data. The network consists of two stages(Fig.3).

1. The first stage is a deep convolutional encoder-decoder network which **takes an image patch and a trimap as input** and is **penalized by the alpha prediction loss and a novel compositional loss**.
2. The second stage is a small fully convolutional network which refines the alpha prediction from the first network with more accurate alpha values and sharper edges.

We will describe our algorithm with more details in the following sections.

### Matting encoder-decoder stage

![img](assets/2019-01-01-10-19-27.png)

The first stage of our network is a deep encoder-decoder network (see Fig.3), which has achieved successes in many other computer vision tasks such as image segmentation [2], boundary prediction [33] and hole filling [24].

#### Network structure

The input to the network is an **image patch and the corresponding trimap** which are *concatenated along the channel dimension*, resulting in a 4-channel input. The whole network consists of an encoder network and a decoder network.

1. The input to the encoder network is transformed into downsampled feature maps by subsequent convolutional layers and max pooling layers. The decoder network in turn uses subsequent unpooling layers which reverse the max pooling operation and convolutional layers to upsample the feature maps and have the desired output, the alpha matte in our case. Specifically, our encoder net-work has **14 convolutional layers and 5 max-pooling layers**.
2. For the decoder network, we *use a smaller structure than the encoder network* to reduce the number of parametersand speed up the training process. Specifically, our decoder network has **6 convolutional layers, 5 unpooling layers followed by a final alpha prediction layer**.

#### Losses

Our network leverages two losses.

##### alpha-prediction loss

![img](assets/2019-01-01-11-11-18.png)

The first loss is called the **alpha-prediction loss**, which is the absolute difference *between the ground truth alpha values and the predicted alpha values at each pixel*. However, due to the non-differentiable property of absolute values, we use the following loss function to approximate it.

![img](assets/2019-01-01-10-34-15.png)

* $Î±^i_p$ is the output of the prediction layer at pixel $i$ thresholded between 0 and 1.
* $Î±^i_g$ is the ground truth alpha value at pixel $i$.
* $\epsilon$is a small value which is equal to $10^{âˆ’6}$ in our experiments.
* The derivative $âˆ‚L^i_Î±/âˆ‚Î±^i_p$ is straightforward(ç›´æˆªäº†å½“).

![img](assets/2019-01-01-10-37-52.png)

##### compositional loss

![img](assets/2019-01-01-11-10-47.png)

The second loss is called the **compositional loss**, which is the absolute difference *between the ground truth RGB colors and the predicted RGB colors* **composited by the ground truth foreground, the ground truth background and the predicted alpha mattes**. Similarly, we approximate it by usingthe following loss function.

![img](assets/2019-01-01-10-49-06.png)

* $c$ denotes the RGB channel
* $p$ denotes the image composited by the predicted alpha
* $g$ denotes the image composited by the ground truth alphas

The compositional loss constrains the network to follow the compositional operation, leading to more accurate alpha predictions.

æ€»æŸå¤±æ˜¯ä¸¤ä¸ªå•ç‹¬æŸå¤±çš„åŠ æƒæ€»å’Œ, i.e., $L_{overall}=w_lÂ·L_Î±+(1âˆ’w_l)Â·L_c$.

* $w$ lis set to 0.5 in our experiment.

In addition, since *only the alpha values inside the unknown regions of trimaps need to be inferred*, we therefore set additional weights on the two types of losses *according to the pixel locations*, which can help our network pay more attention on the important areas. (æ‰€è°“æ³¨æ„åŠ›æœºåˆ¶?)

Specifically, $w_i= 1$ if pixel $i$ is inside the unknown region of the trimap while $w_i= 0$ otherwise.

#### Implementation

##### Train

Although our training dataset has 49,300 images, there are **only 493 unique objects**. **To avoid overfitting as well as to leverage the training data more effectively**, we use several training strategies.

1. First, we **randomly crop 320Ã—320 (image, trimap) pairs centered on pixels in the unknown regions**. This increases our sampling space.
2. Second, we also **crop training pairs with different sizes (e.g. 480Ã—480, 640Ã—640) and resize them to 320Ã—320**. This makes our method more robust to scales and helps the network better learn context and semantics.
3. Third, **flipping is performed randomly on each *training pair*.
4. Fourth, the **trimaps are randomly dilated(éšæœºæ‰©å¼ ) from their ground truth alpha mattes**, helping our model to be more robust to the trimap placement.
5. Finally, **the training inputs are recreated randomly after each training epoch**.

The encoder portion of the network is initialized with the first 14 convolutional layers of VGG-16 [30] (*the 14th layeris the fully connected layer â€œfc6â€ which can be transformed to a convolutional layer*).

Since the network has 4-channel input, we initialize the one extra channel of the first-layer convolutional filters with zeros.

> è¿™é‡Œæ€ä¹ˆå®ç°? é’ˆå¯¹ä¸€éƒ¨åˆ†æƒé‡ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹, å‰©ä½™çš„é›¶åˆå§‹åŒ–?

All the decoder parameters are initialized with Xavier random variables.

##### Test

* the image and corresponding trimap are concatenated as the input.
* A forward pass of the network is performed to output the alpha matte prediction.
* When a GPU memory is in sufficient for large images, CPU testing can be performed.

### Matting refinement stage

Although the alpha predictions from the first part of our network are already much better than existing matting algorithms, because of the encoder-decoder structure, the results are sometimes overly smooth.

Therefore, we extend our network to further refine the results from the first part. This extended network usually predicts **more accurate alpha mattes and sharper edges**.

#### Network structure

* The input to the second stage of our network is **the concatenation of an image patch and its alpha prediction** from the first stage (scaled between 0 and 255), resulting in a 4-channel input.
* The output is the corresponding ground truth alpha matte.

The network is a **fully convolutional network which includes 4 convolutional layers. Each of the first 3 convolutional layers is followed by a nonlinear â€œReLUâ€ layer**. There are no downsampling layers since we want to keep very subtle(ç»†å¾®) structures missed inthe first stage.

In addition, we use a â€œskip-modelâ€ structure **where the 4-th channel of the input data is first scaled between 0 and 1 and then is added to the output of the network**.

![img](assets/2019-01-01-11-12-29.png)

> è¿™é‡Œçš„æè¿°å’Œå›¾ä¸Šæœ‰äº›å‡ºå…¥, å› ä¸ºæŒ‰ç…§å›¾ä¸Šæ¥çœ‹, æ˜¯è¿™ä¸ª4é€šé“åªæ˜¯ä½œä¸ºäº†è¾“å…¥, è€ŒåæœŸåŠ ä¸Šæ¥çš„, æ˜¯ä¸€ä¸ªé¢„æµ‹å‡ºæ¥çš„alpha matte. åº”è¯¥æ˜¯ç”¨å“ªä¸ª?

The detailed configuration is shown in Fig. 3. The effect of our refinement stage is illustrated in Fig. 4. Note that it does not make large-scale changes to the alphamatte, but rather just refines and sharpens the alpha values.

![img](assets/2019-01-01-11-55-00.png)

#### Implementation

1. During training, we first update the encoder-decoder part without the refinement part.
2. After the encoder-decoder part is converged(èåˆ), we fix its parameters and then update the refinement part. Only the alpha prediction loss (Eqn.2) is used due to its simple structure. We also use all the training strategies of the $1^{st}$ stage except the $4^{th}$ one.
3. After the refinement part is also converged, finally we fine-tune the the whole network together. (åˆ†é˜¶æ®µè®­ç»ƒ)

We use Adam [20] to update both parts. A small learning rate $10^{âˆ’5}$ is set constantly during the training process.

During testing

1. first, given an image and a trimap, our algorithm first uses the matting encoder-decoder stage to get an initial alpha matte prediction.
2. then the image and the alph aprediction are concatenated as the input to the refinement stage to produce the final alpha matte prediction.

## Experimental results

In this section we evaluate our method on 3 datasets.

* We evaluate on the alphamatting.com dataset [25], which is the existing benchmark for image matting methods. It includes 8 testing images, each has 3 different trimaps, namely, â€œsmallâ€, â€œlargeâ€ and â€œuserâ€.
* Due to the limited size and range of objects in the alphamatting.com dataset, we propose the Composition-1k test set. Our composition-based dataset includes 1000 images and 50 unique foregrounds. This dataset has a wider range of object typesand background scenes.
* To measure our performance on natural images, we also collect a third dataset including 31 natural images. The natural images cover a wide range ofcommon matting foregrounds such as person, animals, etc.

### The alphamatting.com dataset

Our approach achieves the top results compared to all the other methods on the alphamatting.com benchmark. Specifically, our method ranks the 1st place in terms of the SAD metric. Our method also has the smallest SAD errors for 5 images with all the 3 trimaps (Fig. 5).

![img](assets/2019-01-01-11-25-02.png)

In addition, our method ranks the 2nd place in terms of both the MSE and Gradient metrics. Overall, our method is one of the best performers on this dataset.

A key reason for our success is our networkâ€™s ability to learn structure and semantics, which is important for the accurate estimation of alpha matte when **the background scene is complex or the background and foreground colors are similar**.

For example, in Fig6 the â€œTrollâ€ examplehas very similar colors of the hair and the bridge while the â€œDollâ€ example has strong textured background.

![img](assets/2019-01-01-11-26-50.png)

The best results of previous methods (from column 3 to column 6) all have very obvious mistakes in those hard regions.

In contrast, our method directly learns object structure and image context.

As a result, our method not only avoids the similar mistakes made by previous methods but also predicts more details. It is worth noting that although DCNN matting [8] is also a deep-learning based method, **it learns the non-linear combination of previous matting methods within small local patches**. Therefore the method cannot really understand semantics and thus has the same limitations as previous non-deep-learning-based methods.

### The Composition-1k testing dataset

We further evaluate 7 top performing prior methods and each component of our approach on the Composition-1k testing dataset. For all prior methods, the authorsâ€™ provided codes are used. The different variants of our approach include: the matting encoder-decoder network 1) with only the alpha prediction loss, 2) with both the alpha prediction loss and the compositional loss, the matting encoder-decoder network 3) post-processed by the Guided filter [17] and 4) post-processed by the matting refinement network.

The quantitative results under the SAD, MSE, Gradient and Connectivity errors proposed by [25] are displayed in Table 1.

![img](assets/2019-01-01-11-33-28.png)

Clearly all variants of our approach have much better results than the other methods. The main reason is still the capability of our deep model understanding the complex context of images while the other methods cannot.

By comparing the variants of our approach, we can also validate the effectiveness of each component of our approach:

1. the compositional loss helps our model learn the compositional operation, and thus leads to better results
2. the results of our matting encoder-decoder network can be improved by combining with previous edge-preserving filters(ä¸å…ˆå‰çš„è¾¹ç¼˜ä¿ç•™æ»¤æ³¢å™¨ç›¸ç»“åˆ) (e.g. Guided filter [17]) as well as our matting refinement network. But the latter one has more obvious improvement both visually and quantitatively since it is directly trained with the outputs of our encoder-decoder network.

![img](assets/2019-01-01-11-37-03.png)

We test the sensitivity of our method to trimap placement in Fig. 7. We evaluate over a subset of our dataset that includes one randomly-chosen image for each unique object for a total of 50 images. To form the trimap, we dilate the ground truth alpha for each image by $d$ pixels for increasing values of $d$.

The SAD errors at a particular parameter $d$ are averaged over all images. The results of all the methods at parameters $dâˆˆ[1,4,7,10,13,16,19]$ are shown in Fig.7.

Clearly our method has a low and stable error rate with the increasing values of $d$ whiles the error rate of the other approaches increases rapidly.

Our good performance derives from both our training strategies as well as a good understanding of image context.

Some visual examples are shown in Fig.8 to demonstratethe good performance of our approach on different matting cases such as hair, holes and semi-transparency. Moreover, our approach can also handle objects with no pure foreground pixels, as shown in the last example in Fig.8.

![img](assets/2019-01-01-11-42-04.png)

Since **previous sampling-based and propagation-based methods must leverage known foreground and background pixels, they cannot handle this case**, while our approach can learn the appearance of fine details directly from data.

### The real image dataset

Matting methods should generalize well to real-world images. To validate the performance of our approach and other methods on real images, we conduct a user study on the real image dataset. These images consist of images pulled from the internet as well as images provided by the ICCV 2013 tutorial on image matting.

Because our subjects may not be acquainted with alpha mattes, we instead evaluate the results of compositions. For each method, the computed alpha matte is used to blend the test image onto a black background and onto a white background.

For the user test, we present the image and the two composition results of two randomly selected approaches to an user and ask which results are more accurate and realistic especially in the regions of fine details (e.g. hair, edgesof object, and semi-transparent areas).

To avoid evaluation bias, we conduct the user study on the Amazon Mechanical Turk.

As a result, there are total 392 users participating the user study and each method pair on one image is evaluated by 5 to 6 unique users. The pairwise comparison results are displayed in Tbl.2, where each column presents the preference of one approach over the other methods.

![img](assets/2019-01-01-11-49-04.png)

For example, users preferred our result 83.7% of the time over [13].

Notably almost 4 out of 5 users prefer our method over the prior methods, which well demonstrates that our method indeed produces better visual results. See Fig.9 for some visual results.

![img](assets/2019-01-01-11-50-19.png)

It is also worth noting that the ranking of other methods differs in this test compared to the other two experiments.

For example, Closed-Form Matting [22] is the lowest ranked method on alphamatting.com of the methods we compare here, yet to users it is preferable to all other methods except our own and [28].

On the other hand, while DCNN [8] is the prior state-of-the-art method on alphamatting.com, is only preferred over two methods on the real images. It is unclear whether this is due to methods overfitting the alphamatting.com dataset or whether the standard error metrics fail to accurately measure human perceptual judgment of alpha matting results.

## Conclusion

In order to generalize to natural images, matting algorithms must move beyond using color as a primary cue and leverage more structural and semantic features.

In this work, **we show that a neural network is capable of capturing such high-order features and applying them to compute improved matting results**.

Our experiments show that our method does not only outperform prior methods on the standard dataset, but that it generalizes to real images significantly better as well.
